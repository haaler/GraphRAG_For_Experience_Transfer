{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98efa5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import uuid\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513f07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder = \"data_json/\"\n",
    "\n",
    "def load_documents_one_per_pdf(pdf_folder):\n",
    "    all_docs = []\n",
    "    \n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.endswith(\".PDF\"):\n",
    "            loader = PyPDFLoader(os.path.join(pdf_folder, filename))\n",
    "            docs = loader.load()\n",
    "            \n",
    "            full_text = \" \".join([doc.page_content for doc in docs])\n",
    "            new_doc = Document(\n",
    "                page_content=full_text,\n",
    "                metadata={\"source_file\": filename}\n",
    "            )\n",
    "            all_docs.append(new_doc)\n",
    "    \n",
    "    return all_docs\n",
    "\n",
    "documents = load_documents_one_per_pdf(pdf_folder)\n",
    "print(f\"Loaded {len(documents)} documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2f73be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\DKAR\\AppData\\Local\\Temp\\ipykernel_5344\\2846275196.py:12: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  re.search(r\"\\d+\\s+\\d+\\s+\\d+\", line) or\n"
     ]
    }
   ],
   "source": [
    "\"\"\" def clean_text(documents):\n",
    "    cleaned_documents = []\n",
    "\n",
    "    for doc in documents:\n",
    "        text = doc.page_content\n",
    "\n",
    "        lines = text.splitlines()\n",
    "        filtered_lines = []\n",
    "\n",
    "        for line in lines:\n",
    "            if (\n",
    "                re.search(r\"\\d+\\s+\\d+\\s+\\d+\", line) or\n",
    "                line.count(\"|\") > 1 or\n",
    "                len(re.findall(r\"\\d\", line)) > 15\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            filtered_lines.append(line)\n",
    "\n",
    "        text = \" \".join(filtered_lines)\n",
    "\n",
    "        text = re.sub(r\"\")\n",
    "\n",
    "        doc.page_content = text\n",
    "        cleaned_documents.append(doc)\n",
    "\n",
    "    return cleaned_documents\n",
    "\n",
    "\n",
    "documents = clean_text(documents)\n",
    " \"\"\"\n",
    "\n",
    "def clean_text(documents):\n",
    "    cleaned_docs = []\n",
    "\n",
    "    for doc in documents:\n",
    "        lines = doc.page_content.splitlines()\n",
    "        kept_lines = []\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            num_ratio = sum(c.isdigit() for c in line) / max(len(line), 1)\n",
    "            many_spaces = line.count(\"  \") >= 2\n",
    "\n",
    "            if num_ratio > 0.35 and many_spaces:\n",
    "                continue\n",
    "\n",
    "            kept_lines.append(line)\n",
    "\n",
    "        text = \"\\n\".join(kept_lines)\n",
    "\n",
    "        text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "        text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "\n",
    "        doc.page_content = text\n",
    "        cleaned_docs.append(doc)\n",
    "\n",
    "    return cleaned_docs\n",
    "\n",
    "documents = clean_text(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df1bfd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 96 chunks\n"
     ]
    }
   ],
   "source": [
    "def chunk_documents(documents, chunk_size=1000, chunk_overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\n",
    "            \"\\n\\n\",   # seksjoner\n",
    "            \"\\n\",\n",
    "            \". \", \"? \", \"! \",\n",
    "            \" \"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    all_chunks = []\n",
    "\n",
    "    for doc in documents:\n",
    "        document_id = doc.metadata.get(\"document_id\")\n",
    "        if not document_id:\n",
    "            document_id = str(uuid.uuid4())\n",
    "            doc.metadata[\"document_id\"] = document_id\n",
    "\n",
    "        text = doc.page_content\n",
    "        text = re.sub(r\"\\s(\\d+\\s+[A-Z][A-Z\\s]+)\", r\"\\n\\n\\1\", text)\n",
    "\n",
    "        doc.page_content = text\n",
    "\n",
    "        doc_chunks = splitter.split_documents([doc])\n",
    "\n",
    "        for i, chunk in enumerate(doc_chunks, start=1):\n",
    "            chunk.metadata.clear()\n",
    "            chunk.metadata[\"document_id\"] = document_id\n",
    "            chunk.metadata[\"chunk_index\"] = i\n",
    "            chunk.metadata[\"source_file\"] = doc.metadata.get(\"source_file\")\n",
    "\n",
    "        all_chunks.extend(doc_chunks)\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "chunks = chunk_documents(documents)\n",
    "print(f\"Created {len(chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9598a5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "serializable_chunks = [\n",
    "    {\"page_content\": c.page_content, \"metadata\": c.metadata}\n",
    "    for c in chunks\n",
    "]\n",
    "\n",
    "with open(\"chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(serializable_chunks, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99c6678a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS vectorstore lagret.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"embeddinggemma\")\n",
    "\n",
    "texts = [c.page_content for c in chunks]\n",
    "metadatas = [c.metadata for c in chunks]\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embeddings,\n",
    "    metadatas=metadatas\n",
    ")\n",
    "\n",
    "vectorstore.save_local(\"faiss_chunks\")\n",
    "print(\"FAISS vectorstore lagret.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8841f703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document_id': '777e70ce-dca4-4c8a-83aa-9d48758e19a4', 'chunk_index': 9, 'source_file': '31028-J-KB-0001_01_004.PDF'}\n",
      "3 MAP AND LOCATION SPECIFIC REQUIREMENTS\n",
      "3.1 Location Data\n",
      "Figure 3-1: Location Area Map, Askeladd South, Template L, ref. /1/. Input to WSOG\n",
      "DSA @ Askeladd South – Template L\n",
      "Document no: 31028-J-KB-\n",
      "-----\n",
      "{'document_id': '777e70ce-dca4-4c8a-83aa-9d48758e19a4', 'chunk_index': 10, 'source_file': '31028-J-KB-0001_01_004.PDF'}\n",
      "Northwest\n",
      "50m starboard direction\n",
      "Water depth 245 m\n",
      "Easting 478 190 m\n",
      "Southing 7 915 792 m\n",
      "ED1950, UTM Zone 34N Input to WSOG\n",
      "DSA @ Askeladd South – Template L\n",
      "Document no: 31028-J-KB-0001 Rev: 01 Dat\n",
      "-----\n",
      "{'document_id': '777e70ce-dca4-4c8a-83aa-9d48758e19a4', 'chunk_index': 19, 'source_file': '31028-J-KB-0001_01_004.PDF'}\n",
      "5 60 222 222 278 278\n",
      "6 60 212 212 265 265\n",
      "7 60 222 222 277 277\n",
      "8 60 210 210 263 263\n",
      "Askeladd South Template L\n",
      "WSOG Tension Table\n",
      "Mooring\n",
      "Line No [tonnes] [tonnes]\n",
      "35.3\n",
      "Green\n",
      "[Between]\n",
      "Advisory\n",
      "[Betwee\n",
      "-----\n",
      "{'document_id': '777e70ce-dca4-4c8a-83aa-9d48758e19a4', 'chunk_index': 11, 'source_file': '31028-J-KB-0001_01_004.PDF'}\n",
      "4 MOORING LINE DATA\n",
      "4.1 Mooring Spread\n",
      "700m / 84mm\n",
      "037.5\n",
      "o à\n",
      "700m / 84mm\n",
      "ß\n",
      "352.5o\n",
      "700m / 84mm\n",
      "082.5\n",
      "o à\n",
      "700m / 84mm\n",
      "127.5 o\n",
      "à700m / 84mm\n",
      "172.5o à\n",
      "700m / 84mm\n",
      "ß 262.5\n",
      "o\n",
      "700m / 84mm\n",
      "ß 307.5 o\n",
      "Rig Headin\n",
      "-----\n",
      "{'document_id': '777e70ce-dca4-4c8a-83aa-9d48758e19a4', 'chunk_index': 6, 'source_file': '31028-J-KB-0001_01_004.PDF'}\n",
      "1 GENERAL\n",
      "This document gives the required input for establishing the WSOG for the Askeladd\n",
      "South location. In addition, the document contains information and guidelines for safe\n",
      "operation of the moor\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the rig heading of Askeladd South?\"\n",
    "results = vectorstore.similarity_search(query, k=5)\n",
    "\n",
    "for r in results:\n",
    "    print(r.metadata)\n",
    "    print(r.page_content[:200])\n",
    "    print(\"-----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ac6012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "vectorstore = FAISS.load_local(\"faiss_chunks\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "llm = ChatOllama(model=\"qwen3:8b\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a26d256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Du er en presis assistent. Bruk KUN konteksten under for å svare.\n",
    "\n",
    "Kontekst:\n",
    "{context}\n",
    "\n",
    "Spørsmål:\n",
    "{question}\n",
    "\n",
    "Svar:\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "232ea257",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf9e5b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**Svar:**  \\nTotal line (chain) length på line 5 på Breidablikk M-template er **1536 meter**, som angitt i tabellen i dokumentet med ID `9f39b715-c172-4b9d-9049-731ee57febe9`.' additional_kwargs={} response_metadata={'model': 'qwen3:8b', 'created_at': '2025-11-28T11:04:43.4384274Z', 'done': True, 'done_reason': 'stop', 'total_duration': 671104259800, 'load_duration': 5413816400, 'prompt_eval_count': 2250, 'prompt_eval_duration': 477675771700, 'eval_count': 469, 'eval_duration': 187744804700, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'} id='lc_run--f8d92247-4a9d-40cf-992f-56c001ee98f7-0' usage_metadata={'input_tokens': 2250, 'output_tokens': 469, 'total_tokens': 2719}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is total line (chain) lenght on line 5 on Breidablikk M-template?\"\n",
    "\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAT300",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
