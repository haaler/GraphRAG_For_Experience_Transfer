{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98efa5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "513f07db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 documents.\n"
     ]
    }
   ],
   "source": [
    "pdf_folder = \"data_json/\"\n",
    "\n",
    "def load_documents_one_per_pdf(pdf_folder):\n",
    "    all_docs = []\n",
    "    \n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.endswith(\".PDF\"):\n",
    "            loader = PyPDFLoader(os.path.join(pdf_folder, filename))\n",
    "            docs = loader.load()\n",
    "            \n",
    "            # Slå sammen alt innhold fra PDF-en til ett Document\n",
    "            full_text = \" \".join([doc.page_content for doc in docs])\n",
    "            new_doc = Document(\n",
    "                page_content=full_text,\n",
    "                metadata={\"source_file\": filename}\n",
    "            )\n",
    "            all_docs.append(new_doc)\n",
    "    \n",
    "    return all_docs\n",
    "\n",
    "documents = load_documents_one_per_pdf(pdf_folder)\n",
    "print(f\"Loaded {len(documents)} documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2f73be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(documents):\n",
    "    cleaned_documents = []\n",
    "\n",
    "    for doc in documents:\n",
    "        raw_text = doc.page_content\n",
    "\n",
    "        # Del i linjer for å kunne filtrere tabellrader\n",
    "        lines = raw_text.splitlines()\n",
    "        filtered_lines = []\n",
    "\n",
    "        for line in lines:\n",
    "            # Heuristikk for å hoppe over tabeller:\n",
    "            # mange | eller mange mellomrom/kolonnestruktur\n",
    "            if (\n",
    "                line.count(\"|\") > 2 or\n",
    "                re.search(r\"\\s{3,}\", line) or\n",
    "                re.search(r\";\", line)\n",
    "            ):\n",
    "                continue  # hopp over tabellrad\n",
    "\n",
    "            filtered_lines.append(line)\n",
    "\n",
    "        text = \" \".join(filtered_lines)\n",
    "\n",
    "        # Normal rensing\n",
    "        text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        doc.page_content = text\n",
    "        cleaned_documents.append(doc)\n",
    "\n",
    "    return cleaned_documents\n",
    "\n",
    "\n",
    "documents = clean_text(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df1bfd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 79 chunks\n"
     ]
    }
   ],
   "source": [
    "def chunk_documents(documents, chunk_size=1000, chunk_overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "    all_chunks = []\n",
    "\n",
    "    for doc in documents:\n",
    "        # Sørg for at hvert dokument har en stabil ID\n",
    "        document_id = doc.metadata.get(\"document_id\")\n",
    "        if not document_id:\n",
    "            document_id = str(uuid.uuid4())\n",
    "            doc.metadata[\"document_id\"] = document_id\n",
    "\n",
    "        doc_chunks = splitter.split_documents([doc])\n",
    "\n",
    "        for i, chunk in enumerate(doc_chunks):\n",
    "            chunk.metadata[\"document_id\"] = document_id\n",
    "            chunk.metadata[\"chunk_index\"] = i\n",
    "\n",
    "        all_chunks.extend(doc_chunks)\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "chunks = chunk_documents(documents)\n",
    "print(f\"Created {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2dac71f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_metadata(chunks):\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk.metadata['chunk_index'] = i\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "chunks = add_metadata(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a04f21fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document No: 31028-J-KB-0001 Odfjell Drilling Document Number: 31028-J-KB-0001 Revision: 01 Date: 17.01.2020 Input to WSOG DSA @ ASKELADD SOUTH TEMPLATE L Input to WSOG DSA @ Askeladd South – Template L Document no: 31028-J-KB-0001 Rev: 01 Date: 17.01.20 Document No: 31028-J-KB-0001 Odfjell Drilling Document history Rev Date Description Prepared Controlled Approved 01A 17.01.2020 Issued for DIC GKVA IDKR CAST 01 17.01.2020 Issued for use GKVA IDKR CAST Document signatures Prepared by: Controlled\n",
      "{'source_file': '31028-J-KB-0001_01_004.PDF', 'document_id': '72188c8e-f595-4cec-a0fa-59477ebfbc61', 'chunk_index': 0}\n"
     ]
    }
   ],
   "source": [
    "print(chunks[0].page_content[:500])\n",
    "print(chunks[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9598a5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "serializable_chunks = [\n",
    "    {\"page_content\": c.page_content, \"metadata\": c.metadata}\n",
    "    for c in chunks\n",
    "]\n",
    "\n",
    "with open(\"chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(serializable_chunks, f, ensure_ascii=False, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAT300",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
